{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18682d20",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2.2em; font-weight:bold; text-align:center;\">Exploratory Data Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e8bbc",
   "metadata": {},
   "source": [
    "This project serves as a **Proof of Concept (PoC)** to explore **anomaly scoring** techniques for detecting potentially fraudulent banking transactions.\n",
    "\n",
    "The data covers the period from **April 2, 2025** to **April 4, 2025**, based on the assumption that in mid-2025, trade tensions lead to increased abnormal or suspicious transaction activities.\n",
    "\n",
    "For exploratory data analysis (EDA), we will:\n",
    "\n",
    "- Integrate dynamic currency conversion using the Riksbanken API.\n",
    "\n",
    "- Explore missing values.\n",
    "\n",
    "- Conduct descriptive analytics using SQL.\n",
    "\n",
    "- Perform univariate and bivariate analysis to analyze data distribution and identify outliers.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d0cf6",
   "metadata": {},
   "source": [
    "| **Key Findings**                                                                                 |\n",
    "|--------------------------------------------------------------------------------------------------|\n",
    "| Dataset merged from `transaction records`, `KYC customer profiles`, and `customer summary statistics`. |\n",
    "| Contains 10,716 transactions with 40 features (31 categorical, 9 numerical).                      |\n",
    "| Includes 107 duplicate records.                                                                  |\n",
    "| All amounts in different currencies have been converted to EUR.                                  |\n",
    "| Missing values are present in some categorical features.                                         |\n",
    "| Several numerical features contain potential outliers.             |\n",
    "| Categorical features exhibit a mix of low and high cardinality.     |       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acec71c",
   "metadata": {},
   "source": [
    "## <h2 style=\"font-size: 1.6em; font-weight: bold;\"> 1. Data Collection </h2>\n",
    "Import Required Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86a414",
   "metadata": {},
   "source": [
    "**Importing Pandas, Numpy, Matplotlib, Seaborn, Plotly.express and Warings Library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d250d",
   "metadata": {},
   "source": [
    "**Import the CSV Data as Pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e36366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data from 3 reports and merge them using a left join on `customer_id`\n",
    "\n",
    "transaction_df = pd.read_csv('data/transactions.csv')\n",
    "customer_df = pd.read_csv('data/customers.csv')\n",
    "sum_statistic_df = pd.read_csv('data/customer_summary_statistics.csv')\n",
    "\n",
    "df = transaction_df.merge(customer_df, on='customer_id', how='left') \\\n",
    "                   .merge(sum_statistic_df, on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f23496",
   "metadata": {},
   "source": [
    "Handle duplicate columns after merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_cols = []\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i + 1, len(cols)):\n",
    "        if df[cols[i]].equals(df[cols[j]]):\n",
    "            dup_cols.append((cols[i], cols[j]))\n",
    "print(\"Columns with identical values:\")\n",
    "for c1, c2 in dup_cols:\n",
    "    print(f\"{c1} and {c2}\")\n",
    "    \n",
    "dropped = set()\n",
    "kept = set()\n",
    "\n",
    "for c1, c2 in dup_cols:\n",
    "    if c2 not in dropped:\n",
    "        df.drop(columns=c2, inplace=True)\n",
    "        dropped.add(c2)\n",
    "        kept.add(c1)     \n",
    "print(\"Dropped columns:\", list(dropped))\n",
    "print (\"Remaining columns:\", list(kept) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    'available_balance_x': 'available_balance',\n",
    "    'country_x' : 'country',\n",
    "    'currency_x': 'currency',\n",
    "    'credit_limit_x' : 'credit_limit'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a6456",
   "metadata": {},
   "source": [
    "**Dataset Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43594295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29fac9",
   "metadata": {},
   "source": [
    "We have a dataset with 10,716 records and 40 columns:\n",
    "\n",
    "  - Categorical features: 31\n",
    "\n",
    "  - Numeric features: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce039c45",
   "metadata": {},
   "source": [
    "## <h2 style=\"font-size: 1.6em; font-weight: bold;\"> 2. Data Cleaning </h2>\n",
    "\n",
    "- Removed Duplicate Entries\n",
    "\n",
    "- Converted amounts to EUR using exchange rates from the Riskbanken open API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653857b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d523055",
   "metadata": {},
   "source": [
    "The data has 107 duplicate records. Drop all duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109813d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Duplicate\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e2c06",
   "metadata": {},
   "source": [
    "Standardize all amount-related columns to EUR to ensure consistency for future analysis.\n",
    "\n",
    "*Using dynamic exchange rates from the Riksbanken API.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabf5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import urllib.request, json\n",
    "\n",
    "\n",
    "# Ensure all currency codes are uppercase\n",
    "currencies = df['currency'].unique()\n",
    "\n",
    "# Build currency-to-series map\n",
    "currency_to_series = {\n",
    "    cur: f'SEK{cur}PMI' if cur != 'SEK' else 'SEK'\n",
    "    for cur in currencies\n",
    "}\n",
    "\n",
    "rate_dict = {}\n",
    "\n",
    "# Loop to fetch exchange rates\n",
    "for cur in currencies:\n",
    "    if cur == 'EUR':\n",
    "        rate_dict[cur] = 1.0  # EUR to EUR\n",
    "        continue\n",
    "    if cur == 'SEK':\n",
    "        series1 = 'SEK'\n",
    "    else:\n",
    "        series1 = currency_to_series[cur]\n",
    "\n",
    "    series1 = currency_to_series[cur]\n",
    "    url = f'https://api.riksbank.se/swea/v1/CrossRates/{series1}/SEKEURPMI/2025-04-4/2025-04-4' \n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.get_method = lambda: 'GET'\n",
    "        response = urllib.request.urlopen(req)\n",
    "        content = response.read()\n",
    "        data = json.loads(content)\n",
    "\n",
    "        # Handle different JSON structures\n",
    "        if isinstance(data, list) and data:\n",
    "            rate = data[0]['value']\n",
    "            rate_dict[cur] = float(rate)\n",
    "        elif isinstance(data, dict) and 'value' in data:\n",
    "            rate_dict[cur] = float(data['value'])\n",
    "        else:\n",
    "            rate_dict[cur] = 1.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching rate for {cur}: {e}\")\n",
    "        rate_dict[cur] = 1.0\n",
    "\n",
    "cols_to_convert = [\n",
    "    'amount', 'available_balance', 'value_dated_balance', 'credit_limit',\n",
    "    'total_volume', 'avg_amount', 'max_amount', 'min_amount'\n",
    "]\n",
    "cols_after_convert = []\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    new_col = col + '_eur'\n",
    "    df[new_col] = df.apply(\n",
    "        lambda x: float(x[col]) if x['currency'].upper() == 'EUR'\n",
    "        else (\n",
    "            float(x[col]) / rate_dict.get('SEK', 1.0) if x['currency'].upper() == 'SEK'\n",
    "            else float(x[col]) * rate_dict.get(x['currency'].upper(), 1.0)\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    cols_after_convert.append(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (cols_after_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7089009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=cols_to_convert, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/RAW_transaction_monitoring_merged.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec11d2",
   "metadata": {},
   "source": [
    "## <h2 style=\"font-size: 1.6em; font-weight: bold;\"> 3. EDA </h2>\n",
    "\n",
    "- Explored missing values.\n",
    "\n",
    "- Conducted descriptive analytics using SQL.\n",
    "\n",
    "- Performed univariate and bivariate analysis.\n",
    "\n",
    "- Analyzed data distribution and identified outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd383c5",
   "metadata": {},
   "source": [
    "**3.1 Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3282ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pct = round(df.isnull().sum()*100/len(df),2)\n",
    "missing_column_pct = missing_pct[missing_pct> 0.1]\n",
    "missing_column_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132686f7",
   "metadata": {},
   "source": [
    "We have a high percentage of missing values, mostly in categorical features. So, during feature engineering, we will replace them with a new label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64aceb",
   "metadata": {},
   "source": [
    "**3.2 Descriptive Analytics using SQL** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(':memory:')\n",
    "df.to_sql('RAW_transaction_monitoring_merged', con, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda79db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select count (*) as total_transactions , count(distinct customer_id) as total_customers,\n",
    "count(distinct currency) as currencies, Min(amount_eur) as Min_transaction, Max(amount_eur) as Max_transaction,\n",
    "AVG (amount_eur) as avg_transaction\n",
    "from RAW_transaction_monitoring_merged\n",
    "'''\n",
    "df_result = pd.read_sql_query(query,con)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f138c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select 'Lowest_traffic_weekday' as category, weekday, transaction_count\n",
    "from(\n",
    "select strftime ('%w',transaction_date) as weekday, count(*) as transaction_count\n",
    "from RAW_transaction_monitoring_merged \n",
    "group by strftime ('%w',transaction_date) \n",
    "order by count(*) asc\n",
    "limit 1)\n",
    "\n",
    "\n",
    "UNION ALL \n",
    "select 'Highest_traffic_weekday' as category, weekday, transaction_count\n",
    "from(\n",
    "select strftime ('%w',transaction_date) as weekday, count(*) as transaction_count\n",
    "from RAW_transaction_monitoring_merged \n",
    "group by strftime ('%w',transaction_date) \n",
    "order by count(*) desc\n",
    "limit 1)\n",
    "\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql_query(query,con)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403a14d",
   "metadata": {},
   "source": [
    "**Descriptive Overview:**\n",
    "\n",
    "- Total Records: **10,609** transactions\n",
    "\n",
    "- Customers: **1,000** unique customer IDs\n",
    "\n",
    "- Currencies: 4 types used\n",
    "\n",
    "- Minimum Transaction Amount: €1.1\n",
    "\n",
    "- Maximum Transaction Amount: €19,659.95\n",
    "\n",
    "- Average Transaction Amount: €2. 618.77\n",
    "\n",
    "- Transactions had the lowest traffic on Wednesday and the highest traffic on Thursday.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f21e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= '''\n",
    "select 'Least_frequent_customer' as category, customer_id, transaction_count, total_amount\n",
    "from \n",
    "(select *, sum(amount_eur) as total_amount, count(*) as transaction_count\n",
    "from RAW_transaction_monitoring_merged\n",
    "group by customer_id\n",
    "order by count(*) asc\n",
    "limit 1\n",
    ")\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "select 'Most_frequent_customer' as category, customer_id, transaction_count, total_amount\n",
    "from \n",
    "(select *, sum(amount_eur) as total_amount,count(*) as transaction_count\n",
    "from RAW_transaction_monitoring_merged\n",
    "group by customer_id\n",
    "order by count(*) desc\n",
    "limit 1\n",
    ")\n",
    "\n",
    "'''\n",
    "df_result = pd.read_sql_query(query,con)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select avg_amount_per_customer, rank\n",
    "from\n",
    "(select AVG(amount_eur) AS avg_amount_per_customer, rank() OVER(Order by  AVG(amount_eur) desc) as rank\n",
    "FROM RAW_transaction_monitoring_merged\n",
    "group by customer_id\n",
    "order by  AVG(amount_eur) desc)\n",
    "where rank = 1\n",
    "or rank = 1000\n",
    "'''\n",
    "\n",
    "df_result = pd.read_sql_query(query,con)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39420e6",
   "metadata": {},
   "source": [
    "**Customer Behavior Insights:**\n",
    "\n",
    "   - Each customer made between 1 and 23 transactions.\n",
    "   \n",
    "   - Customer `CUST00273` made the least frequent transaction (1) with an amount of €210, while `CUST00797` made the most frequent transactions (23) with a total of €155,607.\n",
    "\n",
    "   - Average transaction amount per customer ranges from approximately €18 to €14,306."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2881753",
   "metadata": {},
   "source": [
    "**3.3 Univariate and Bivariate analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1770c0",
   "metadata": {},
   "source": [
    "**Numeric Features (Continuous)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049628a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_feature = df.select_dtypes(include='float').columns.to_list()\n",
    "print(continuous_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s explore these Continuous features further.\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(continuous_feature,1):\n",
    "      df_copy = df.copy()\n",
    "      plt.subplot(4, 2, i)\n",
    "      sns.histplot(df_copy[col], bins=50, kde=True, color='darkblue')\n",
    "      plt.title(f'Distribution of {col}')\n",
    "      plt.xlabel(col)\n",
    "      plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07619297",
   "metadata": {},
   "source": [
    "Most of the columns are skewed, so transformation is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c182d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_con = df[continuous_feature].corr()\n",
    "corr_matrix_con"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934b372",
   "metadata": {},
   "source": [
    "These independent variables are highly correlated; we may want to perform feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff59147",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "\n",
    "for i,col in enumerate(continuous_feature,1):\n",
    "  df_copy = df.copy()\n",
    "  plt.subplot (4,2,i)\n",
    "  sns.boxplot(df_copy[col], color='darkblue')\n",
    "  plt.title(f\"Outlier of {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49dc488",
   "metadata": {},
   "source": [
    "Most columns have outliers. We need to be mindful of this when handling missing data, especially if there are any continuous features. Additionally, we need to apply some methods to handle these outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4012474a",
   "metadata": {},
   "source": [
    "**Temporal Variables (Datetime Variables)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7beb8",
   "metadata": {},
   "source": [
    "We identify datetime columns from object types, convert them to the correct datetime format, and then transform them into numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ee5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns= {'latest_transaction_booking_date': 'last_booking_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "date_col = []\n",
    "\n",
    "for col in df.columns: \n",
    "    if df[col].dtypes == 'O':\n",
    "      try:\n",
    "         convert = pd.to_datetime(df[col])\n",
    "         if is_datetime64_any_dtype(convert):\n",
    "            if col != 'dob':\n",
    "               new_name = col+'time'\n",
    "               df.rename(columns={col:new_name}, inplace=True)\n",
    "               date_col.append(new_name)\n",
    "            else: \n",
    "              date_col.append(col)\n",
    "      except Exception: \n",
    "         continue\n",
    "print (date_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355475c",
   "metadata": {},
   "source": [
    "Since the data retrieves transactions within a 2-day period (April 2–4, 2025), we will not extract the month and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc73733",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_date_cols = []\n",
    "for col in date_col[:]:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "    if col != 'dob':\n",
    "      df[col + '_day'] = df[col].dt.day\n",
    "      df[col + '_weekday'] = df[col].dt.day_of_week\n",
    "      df[col + 'hour'] = df[col].dt.hour\n",
    "      df[col + 'minute'] = df[col].dt.minute \n",
    "      df[col + 'sec'] = df[col].dt.second\n",
    "      new_date_cols.extend([col + '_day', col + '_weekday'])\n",
    "\n",
    "print(new_date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d849e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i,col in enumerate(new_date_cols,1):\n",
    "    df_copy = df.copy\n",
    "    plt.subplot(11,2,i)\n",
    "    sns.countplot(x=col, data=df, palette='viridis')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de93e77",
   "metadata": {},
   "source": [
    "The columns `last_booking_datetime_day` and `last_booking_datetime_weekday` only have one unique value, so they provide no additional predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ef682",
   "metadata": {},
   "source": [
    "**Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00544360",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include='object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc581a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the number of unique categories in each categorical (object) column.\n",
    "for feature in categorical_features: \n",
    "    count_value = df[feature].nunique()\n",
    "    print (feature,'-', count_value,'categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/EDA_transaction_monitoring_merged.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
